# Ollama configuration
Agent:
  service: ollama

# Model to use for chat
model: dolphin-llama3

# Ollama API host (default is usually fine)
host: http://localhost:11434

# Generation parameters
temperature: 0.7
top_p: 1.0
top_k: 40

# You can add more parameters here as needed, such as:
# max_tokens: 100
# stop_sequences: ["\n", "Human:", "AI:"]